<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>CPU Benchmark Performance: AI and Inferencing | ZestVibe</title><meta name=generator content="Hugo 0.98.0"><meta name=description content="As technology progresses at a breakneck pace, so too do the demands of modern applications and workloads. With artificial intelligence (AI) and machine learning (ML) becoming increasingly intertwined with our daily computational tasks, it's paramount that our reviews evolve in tandem. Recognizing this, we have AI and inferencing benchmarks in our CPU test suite for 2024. 
Traditionally, CPU benchmarks have focused on various tasks, from arithmetic calculations to multimedia processing. However, with AI algorithms now driving features within some applications, from voice recognition to real-time data analysis, it's crucial to understand how modern processors handle these specific workloads."><link rel=stylesheet href=https://assets.cdnweb.info/hugo/cayman/css/normalize.css><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,700" rel=stylesheet type=text/css><link rel=stylesheet href=https://assets.cdnweb.info/hugo/cayman/css/cayman.css><link rel=apple-touch-icon sizes=180x180 href=./apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=./favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=./favicon-16x16.png><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css integrity=sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js integrity=sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js integrity=sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI crossorigin=anonymous onload=renderMathInElement(document.body)></script></head><body><section class=page-header><h1 class=project-name>ZestVibe</h1><h2 class=project-tagline></h2><nav><a href=./index.html class=btn>Blog</a>
<a href=./sitemap.xml class=btn>Sitemap</a>
<a href=./index.xml class=btn>RSS</a></nav></section><section class=main-content><h1>CPU Benchmark Performance: AI and Inferencing</h1><div><strong>Publish date: </strong>2024-08-06</div><p>As technology progresses at a breakneck pace, so too do the demands of modern applications and workloads. With artificial intelligence (AI) and machine learning (ML) becoming increasingly intertwined with our daily computational tasks, it's paramount that our reviews evolve in tandem. Recognizing this, we have AI and inferencing benchmarks in our CPU test suite for 2024.&nbsp;</p><p>Traditionally, CPU benchmarks have focused on various tasks, from arithmetic calculations to multimedia processing. However, with AI algorithms now driving features within some applications, from voice recognition to real-time data analysis, it's crucial to understand how modern processors handle these specific workloads. This is where our newly incorporated benchmarks come into play.</p><p>As chip makers such as AMD with Ryzen AI and Intel with their Meteor Lake mobile platform feature AI-driven hardware within the silicon, it seems in 2024, and we're going to see many applications using AI-based technologies coming to market.</p><p>We are using DDR5 memory on the Core i9-14900KS, as well as the other Intel 14th&nbsp;Gen Core series processors including the Core i9-14900K, the&nbsp;Core i7-14700K, Core i5-14600K, and Intel's 13th Gen at the relative JEDEC settings. The same methodology is also used for the AMD Ryzen 7000 series and Intel's 12th Gen (Alder Lake) processors. Below are the settings we have used for each platform:</p><ul><li>DDR5-5600B CL46 - Intel 14th & 13th Gen</li><li>DDR5-5200 CL44 - Ryzen 7000</li><li>DDR5-4800 (B) CL40 - Intel 12th Gen</li></ul><p><img alt="(6-1) ONNX Runtime 1.14: CaffeNet 12-int8 (CPU Only)" src=https://cdn.statically.io/img/images.anandtech.com/graphs/graph21378/135496.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></p><p><img alt="(6-1b) ONNX Runtime 1.14: CaffeNet 12-int8 (CPU Only)" src=https://cdn.statically.io/img/images.anandtech.com/graphs/graph21378/135497.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></p><p><img alt="(6-1c) ONNX Runtime 1.14: Super-Res-10 (CPU Only)" src=https://cdn.statically.io/img/images.anandtech.com/graphs/graph21378/135498.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></p><p><img alt="(6-1d) ONNX Runtime 1.14: Super-Res-10 (CPU Only)" src=https://cdn.statically.io/img/images.anandtech.com/graphs/graph21378/135499.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></p><p><img alt="(6-2) DeepSpeech 0.6: Acceleration CPU" src=https://cdn.statically.io/img/images.anandtech.com/graphs/graph21378/135500.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></p><p><img alt="(6-3) TensorFlow 2.12: VGG-16, Batch Size 16 (CPU)" src=https://cdn.statically.io/img/images.anandtech.com/graphs/graph21378/135501.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></p><p><img alt="(6-3b) TensorFlow 2.12: VGG-16, Batch Size 64 (CPU)" src=https://cdn.statically.io/img/images.anandtech.com/graphs/graph21378/135502.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></p><p><img alt="(6-3d) TensorFlow 2.12: GoogLeNet, Batch Size 16 (CPU)" src=https://cdn.statically.io/img/images.anandtech.com/graphs/graph21378/135503.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></p><p><img alt="(6-3e) TensorFlow 2.12: GoogLeNet, Batch Size 64 (CPU)" src=https://cdn.statically.io/img/images.anandtech.com/graphs/graph21378/135504.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></p><p><img alt="(6-3f) TensorFlow 2.12: GoogLeNet, Batch Size 256 (CPU)" src=https://cdn.statically.io/img/images.anandtech.com/graphs/graph21378/135505.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></p><p><img alt="(6-4) UL Procyon Windows AI Inference: MobileNet V3 (float32) " src=https://cdn.statically.io/img/images.anandtech.com/graphs/graph21378/135506.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></p><p><img alt="(6-4b) UL Procyon Windows AI Inference: ResNet 50 (float32) " src=https://cdn.statically.io/img/images.anandtech.com/graphs/graph21378/135507.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></p><p><img alt="(6-4c) UL Procyon Windows AI Inference: Inception V4 (float32) " src=https://cdn.statically.io/img/images.anandtech.com/graphs/graph21378/135508.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></p><p>Regarding AI and inferencing workloads, there is virtually no difference or benefit from going for the Core i9-14900KS over the Core i9-14900K. While Intel takes the win in our TensorFlow-based benchmark, the AMD Ryzen 9 7950X3D, and 7950X both seem to better grasp the type of AI workloads we've tested.</p><p class=postsid style=color:rgba(255,0,0,0)>ncG1vNJzZmivp6x7orrAp5utnZOde6S7zGiqoaenZH9yf5ZxZqKmpJq5bq%2FOq5xmoWlifnWFj2mirGWimsOqsdZmq6GdXajEorqMrKann12ks26%2BwKmrqKpdoa6ssYywoK2gXZZ6tMHPnqlmnpGowW6CjGtkoKCqYsG2vsGoZnA%3D</p><footer class=site-footer><span class=site-footer-credits>Made with <a href=https://gohugo.io/>Hugo</a>. © 2022. All rights reserved.</span></footer></section><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/floating.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/tracking_server_6.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script>var _paq=window._paq=window._paq||[];_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){e="//analytics.cdnweb.info/",_paq.push(["setTrackerUrl",e+"matomo.php"]),_paq.push(["setSiteId","1"]);var e,n=document,t=n.createElement("script"),s=n.getElementsByTagName("script")[0];t.async=!0,t.src=e+"matomo.js",s.parentNode.insertBefore(t,s)}()</script></body></html>